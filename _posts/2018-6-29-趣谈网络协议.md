---
layout: post
title: "趣谈网络协议"
description: "网络"
category: NetWork
tags: [Life]
---

{% include JB/setup %}


--------------------------


# 趣谈网络协议

这篇只是个人的一个总结，原版内容可以查看 [https://time.geekbang.org/column/intro/85](https://time.geekbang.org/column/intro/85) 喜欢可以购买作者专栏，在此做个申明。

**应用层**: `DNS`, `Http`, `Https` 所在的层为应用层.

**传输层**: 应用层封装成包之后会交给传输层（通过`Socket`编程来实现), `传输层`有两种协议：
* 无连接的`UDP`协议
* 面向连接的`TCP`协议
    * `TCP` 协议里有两个端口:
        * 浏览器监听的端口
        * 服务器监听的端口

> 操作系统往往通过端口来判断得到的包应该给哪个进程。

**网络层**: 网络层的协议是IP协议, 会有源`IP`地址和目标`IP`地址。
> 操作系统启动的时候会被 `DHCP` 协议配置`IP`地址, 以及默认网关的IP地址*(192.168.1.1)*.

**MAC层**: 操作系统通过 `ARP` 协议将`IP`地址发送给网关, 将 `IP`包交给下一层 `Mac层`.

* 网关往往是一个路由器, 路由器上维护一张路由表, 路由表由    相邻的路由器通过路由协议得到(常用的有 `OSPF` 和 `BGP`). 最后一个路由器知道这个网络包要去的地方，目标服务器会回复一个 `Mac地址`,  通过这个 `Mac地址`就可以找到目标服务器.

* 目标服务器发现 `Mac地址` 对上， 取下 `Mac头` 发送给操作系统`网络层`, 发现 `IP` 也对上, 取下`IP头`交给`传输层(TCP层)`，传输层对于收到的每个包，都会回复一个包说明收到。如果过一段时间还没到，发送端的`TCP层`会重新发送这个包。

* 当网络包平安到达 `TCP层` 之后, `TCP头`中有目标端口号. 通过 `RPC调用`(`RPC`框架有很多种，有基于`HTTP协议`的放在`HTTP`报文里的，有直接封装在`TCP`报文里的) 来告诉监听这个端口的服务器进程。

## ifconfig 命令
`Windows`上通过`ipconfig`, `Linux`上通过`ifconfig`命令或者`ip addr`命令 `来查看`IP`地址.
> `IP`地址是一个网卡在网络世界的通讯地址，相当于我们现实世界的门牌号。

## 无类型域间选路(CIDR)
将`32`位的`IP`地址分为网络考和主机好两部分。类似于 `10.100.122.2/24`, 前`24`位表示网络号，后`8`位表示主机号.

## 公有IP地址和私有IP地址

![Ip分类]( http://7xpgi9.com1.z0.glb.clouddn.com/IPRange.png "IP分类")

> 公有`IP`地址有个组织统一分配。`192.168.0.x`是最常用的私有`IP`地址.`192.168.0.1`往往就是私有网络的出口地址，例如：家里的电脑连接 `Wi-Fi`, `Wi-Fi` 路由器的地址就是 `192.168.0.1`. `192.168.0.255` 是广播地址, 一旦发送这个地址， 整个`192.168.0`网络里的所有机器都能收到。

## ICMP与ping
`ping` 是基于`ICMP`(`Internet Control Message Protocol`, 互联网控制报文协议)协议工作的. `ICMP` 报文封装在 `IP` 包里面, 有很多的类型，不同类型有不同的代码。最常用的类型是主动请求为`8`, 主动请求的应答为`0`. 常用的`ping`就是查询报文，是一种主动请求，并且获得主动应答的`ICMP`协议.

对`ping`的主动请求，进行抓包称为 `ICMP ECHO REQUEST`.同理主动请求的回复，称为`ICMP ECHO REPLY`.比起原生的`ICMP`, 这里多了两个字段:
* 标识符
* 序号

### ICMP差错报文类型
* 终点不可达`3`, 具体的原因在代码中的标示
    * 网络不可达代码为`0`
    * 主机不可达代码为`1`
    * 协议不可达代码为`2`
    * 端口不可达代码为`3`
    * 需要进行分片但设置了不分片位代码为`4`
* 源抑制`4`
* 超时`11`
* 重定向`5`

### ping: 查询报文类型的使用
`主机A` 的`IP`地址是`192.168.1.1`, `主机B`的`IP`地址是`192.168.1.2`, 在同一子网， `主机A` 运行 `ping 192.168.1.2` 发生了啥？

> 源主机首先会构建一个`ICMP`请求数据包，`ICMP`数据包包含多个字段。最重要的以下两个：
* 类型字段: 对于请求数据包而言该字段为`8`
* 顺序号: 主要用于区分连续`ping`时发出的多个数据包,每发出一个请求数据包，顺序号会自动`+1`.

> 为了能计算往返时间`RTT`, 会在报文的数据部分插入发送时间。然后，由`ICMP`协议将数据包连同地址`192.168.1.2`一同交给`IP层`。`IP层`以`192.168.1.2`作为目的地址，本机`IP地址`作为源地址，加上一些其他控制信息，构建一个`IP数据包`。

### MAC头和IP头的细节
`MAC头`: 里先是`目标MAC地址`，然后是`源MAC地址`和一个协议类型。用来说明里面是`IP`协议。
`IP头`: 里面的版本号， 目前主流的是`IPv4`, 服务类型`TOS`,`TTL`和`8`标识协议.

任何一台机器上，当要访问另一个`IP地址`的时候，都会先判断，这个`目标IP地址`和当前机器的`IP地址`是否在同一网段，需要使用`CIDR`和子网掩码。
* 同一网段：直接将源地址和目标地址放入`IP头`中，然后通过`ARP`获得`MAC地址`，将`源MAC`和`目的MAC`放入`MAC头`中发出去。
* 不同网段: 需要发往默认网关`Gateway`, `Gateway`地址一定和`源IP地址`在同一网段。网关往往是一个路由器，是一个三层转发设备。但是把网关叫做路由器是不完全准确的 `路由器是一台设备，它有五个网口或网卡，相当于有五只手，分别连着五个局域网。每只手的IP地址都和局域网的IP地址在相同的网段，每只手都是它握住哪个局域网的网关`

## UDP 协议
`TCP`是面向连接的， `UDP`是面向无连接的。在互通之前，面向连接的协议会先建立连接，例如：`TCP`会三次握手，而`UDP`不会。

**所谓的建立连接，是为了在客户端和服务端维护连接，而建立一定的数据结构来维护双方交互的状态，用这样的数据结构来保证所谓的面向连接的特性。**
* **`TCP`提供可靠交付**。通过`TCP`连接传输的数据，无差错，不丢失，不重复，并且按序到达。 **`UDP`继承了`IP包`的特性，不保证不丢失，不保证按顺序到达。**
* **`TCP`是面向字节流的**。发送的时候发的是一个流，没头没尾。`IP包`不是一个流，而是一个个`IP包`。之所以变成流，是`TCP`自己状态维护做的事情。**`UDP`继承了`IP`的特性，基于数据报的, 一个个的发，一个个的收。**
* **`TCP`可以有拥塞控制的**。它意识到包丢弃了或者网络环境不好，会根据清空调整自己的行为，看看是不是发快了， 要不要发慢点。 **UDP就不会，应用让我发，我就发。**
* **`TCP`其实是一个有状态服务**， **而`UDP`则是无状态服务**

### UDP包头
当我们发送的`UDP`包到达目标机器后，发现`MAC地址`匹配，于是取下来，将剩下的包传给处理`IP层`的代码。把`IP头`取消来，里面有个`8位`协议，会标示出数据到底是`TCP`还是`UDP`。

无论是`TCP`传数据还是`UDP`传数据，都要监听一个端口。`TCP` 还是 `UDP` 包头里有个端口号， 根据端口号，将数据交给相应的应用程序。

### UDP三大特点 
* **沟通简单**：不需要大量的数据结构，处理逻辑，包头字段。
* **轻信他人**：不会建立连接，虽然有端口号，但是监听在这个地方，谁都可以传给他数据，它也可以传给任何人数据，甚至可以同时传给多人数据。
* **做事不懂权变**：不会根据网络的情况进行发包的拥塞控制，无论网络丢包成啥样，还是该怎么发就怎么发。

### UDP三大使用场景 
* **需要资源少，在网络情况比较好的内网，或者对于丢包不敏感的应用**
* **不需要一对一沟通，建立连接，而是可以广播的应用**
* **需要处理速度快，时延低， 可以容忍少数丢包，但是要求即便网络拥塞，也毫不退缩，一往无前**

### UDP的五个例子
* **网页或者`App`的访问**: `QUIC`(Quick UDP Internet Connections，快速`UDP`互联网连接)是`Google`提出的一种基于`UDP`改进的通信协议，目的是降低网络通信延迟，提供更好的用户互动体验。`QUIC`在应用层上，会自己实现快速连接建立，减少重传时延，自适应拥塞控制。
* **流媒体协议**: 直播协议多使用`RTMP`(也是基于`TCP`的), 由于`TCP`的限制，很多直播应用都基于`UDP`实现自己的视频传输协议。
* **实时游戏**: 要求较为严格，采用自定义的可靠`UDP`协议，自定义重传策略， 能够把丢包产生的延迟降到最低，尽量减少网路问题对游戏性造成的影响。
* **IoT物联网**：一方面物联网领域终端资源少，很可能只是个内存非常小的嵌入式系统，而维护`TCP协议`代价太大；另一方面，物联网对实时性要求也很高，`Google`旗下的`Nest`建立`Thread Group`，推出了物联网通信协议 `Thread` , 就是基于`UDP协议的`
* **移动通行领域**: 在`4G`网络里，移动流量上网的数据面对的协议`GTP-U`是基于`UDP`的。因为移动网络协议比较复杂，而`GTP协议`本身就包含复杂的手机上线下线的通信协议。如果基于`TCP`, `TCP`的机制就显得非常多余。

## TCP 协议
### TCP包头格式 
* **源端口号**和**目标端口号**是不可少的，这一点和`UDP`一样。如果没有这两个端口号，数据就不知道应该发给哪个应用。
* **包的序列号**为了解决乱序的问题。
* **确认序号**发出去的包应该有确认，如果没有收到就应该重新发送，直到送达。这个可以解决不丢包的问题。
> `TCP`是可靠的协议，对于`TCP`来讲，`IP层`丢不丢包我管不着，我自己会通过不断重传保证可靠性。

* **状态位**: `SYN`是发起一个连接，`ACK`是回复, `RST`是重新连接, `FIN`是结束连接。`TCP`是面向连接的，双方要维护连接的状态，这些带状态位的包发送，会引起双方的状态变更。
* **窗口大小**: `TCP`要做**流量控制**，通信双方各申明一个窗口，标示自己当前的处理能力，别发送太快，也别发送太慢。
* **拥塞控制**：对于真正的通路堵不堵车，它无能为力，唯一能做的就是控制自己(控制发送的速度)

`TCP`协议需要重点关注以下几个问题：
 * 顺序问题，稳重不乱
 * 丢包问题，承诺靠谱
 * 连接维护，有始有终
 * 流量控制，把握分寸
 * 拥塞控制，知进退
 
### TCP三次握手
我们假设通路非常不靠谱，`A`要发起一个连接，当发了第一个请求了无音讯的时候，会有很多可能性，比如**第一个请求包丢了**, **超时了**还是**`B`没有响应，不想和我连接**。

`A`不能确认结果，于是再发。终于，又一个请求包到了`B`, 但是请求包到了`B`这个事情，`A`是不知道的，`A`还有可能再发。

`B`收到请求包，就知道`A`的存在，并且知道`A`要和它建立连接。如果`B`不乐意建立连接，则`A`会重试一段时间后放弃，连接建立失败，没有问题；如果`B`乐意建立连接，则会发送应答包给`A`。

对于`B`来说，这个应答包也是不知道能不能到达`A`。这个时候`B`自然不能认为连接建立好了，因为应答包仍然会丢，会绕弯路，或者`A`已经挂了。这个时候`B`还碰到一个诡异的现象，`A`和`B`原来建立连接，做了简单通信后，结束连接。`A`建立连接时，请求包重复发了几次，有的请求包绕了一大圈又回来，`B`会认为这是一个正常的请求，因此建立连接，可以想象这个连接不会进行下去，也没有终止的时候。

`B`发送的应答可能会发送多次，但只要一次到达`A`, `A`就认为连接已经建立，因为对于`A` 来说，他的消息有去有回，`A`会给`B`应答的应答，只有当`B`等到这个消息才会建立连接。
> 三次握手除了双方建立连接外，主要为了沟通**`TCP包`的序号问题**。每个连接都要有不同的序号。这个序号的起始序号是随着时间变化的，可以看成一个`32`位的计数器，每`4ms`加一。

### TCP 四次挥手
`A`开始说"不玩了"，`B`说"知道了"。这个是没有什么问题的，因为在此之前，双方还处于合作状态，如果`A`说"不玩了"，没有收到回复，就会重新发送。但是这个会和结束之后，就可能有异常情况：
* `A`说完"不玩了"之后，直接跑路，就会有问题，`B`还没有发起结束，也得不到回答，`B`就不知道怎么办
* `A`说完"不玩了"，`B`直接跑路，也是有问题的，因为`A`不知道`B`是还是事情要处理，还是过一会儿会发送结束。

> `TCP` 专门设计几种状态来处理这些问题:
* 断开的时候, 当`A`说"不玩了"，就进入`FIN_WAIT_1`的状态，`B`收到`A不玩了`的消息后，发送知道了就进入`CLOSE_WAIT`的状态
* `A`收到`B说知道了`，就进入`FIN_WAIT_2`状态，如果这个时候`B`直接跑路，则`A`将永远在这个状态。`TCP`协议里没有对这个状态的处理，但是`Linux`有，可以调整`tcp_fin_timeout`这个参数， 设置一个超时时间。
* `B`没有跑路，发送了`B也不玩了`的请求到达`A`时，`A`发送"知道B也不玩了"的`ACK`后，从`FIN_WAIT_2`状态结束，按说`A`可以跑路了，但是这个最后的`ACK`万一`B`收不到呢? 则`B`会重新发一个`B不玩了`，这个时候`A`已经跑路了的话，`B`就再也收不到`ACK`了， 因而`TCP协议`要求`A`最后等待一段时间`TIME_WAIT`, 这个时间要足够长，长到如果`B`没有收到`ACK`的话，"B 说不玩了"会重发，`A`会重新发一个`ACK`并且足够时间到达`B`。
* `A` 直接跑路还有一个问题，`A`的端口直接空出来了，但是`B`不知道，`B`发送过来的恶很多包很可能还在路上，如果`A`的端口被一个新的应用占用，这个新的应用就会收到上个连接中`B`发过来的包，虽然序列号是重新生成的，但是这里要上一个双保险，防止产生混乱，因而需要等足够长的时间，等到原来`B`发送的所有包都死翘翘，再空出端口来。等待时间设为`2MSL`(MSL是Maximum Segment Lifetime, 报文最大生存时间)
* 还有个异常就是`B`超过`2MSL`后，依然没有收到它发的`FIN`的`ACK`, 按照`TCP`的原理，`B`当然会重发`FIN`, 这个时候`A`再收到这个包之后，`A`就直接发送`RST`, `B`就知道`A`早跑了。

### 如何实现靠谱的协议
`TCP协议`为了保证顺序性，每一个包都有一个`ID`。在建立连接的时候，会商定起始的`ID`是什么，然后按照`ID`一个个发送。为了保证不丢包，对于发送的包都要进行应答，但是这个应答也不是一个个来的，而是会应答某个之前的`ID`, 表示都收到了, 这种模式称为**累计确认或累计应答**(`cumulative acknowledgment`)。为了记录所有发送和接收的包，`TCP`也需要发送和接收端分别都有缓存来保存这些记录。发送端的缓存里是按照包的`ID`一个个排列，根据处理情况分成`4`个部分：
* 发送并且已经确认的。这部分表示处理完的，应该划掉。
* 发送并尚未确认的。还没有处理完的，需要等待处理完回复之后，才能划掉。
* 没有发送，但是已经等待发送，这部分还没有交代给下属，但是马上要交代的。
* 没有发送，并且暂时也不会发送。这部分表示还没有交代下属，而且暂时也不会交代给下属。
> 第`3`点和第`4`点做区分是为了**流量控制，把握分寸**。在`TCP`里，接收端会给发送端报一个窗口大小(`Advertised Window`), 这个窗口的大小应该等于第`2`点和第`3`点的总和，超过这个窗口，接收端处理不过来就不能发送。

于是发送端需要保持下面的数据结构:
* `LastByteAcked`: 第`1`点和第`2`点的分界线
* `LastByteSent`: 第`2`点和第`3`点的分界线
* `LastByteSend + AdvertisedWindow`: 第`3`点和第`4`点的分界线

对于接收端来说，它的缓存里记录的内容要简单一些：
* **接收并确认过的**: 就是交代给我，并且我做完的。
* **还没接收,但是马上就能接收的**: 就是我自己能够接收的最大工作量
* **还没接收，也没法接收的**: 超出工作量的部分，实在做不完。

> 对应的数据结构就是:
* `MaxRcvBuffer`: 最大缓存的量
* `LastByteRead`: 之后是已经接收了，但还没有被应用层读取的。
* `NextByteExpected`: 第`1`部分和第`2`部分的分界线。

## Socket套接字
`Socket`编程进行端到端通信，往往意识不到中间经过多少局域网，多少路由器，因而能够设置的参数，也只能是端到端协议之上网络层和传输层。
> 在网络层，`Socket`函数需要指定到底是`IPv4`还是`IPv6`, 分别对应设置为`AF_INET`和`AF_INET6`。另外，还要指定到底是`TCP`(**基于数据流*，设置为`SOCK_STREAM`)还是`UDP`(**基于数据报**,设置为`SOCK_DGRAM`)。

### 基于TCP协议的Socket程序函数调用过程
两端创建`Socket`之后, `TCP`服务端先监听一个端口，一般先调用`bind函数`, 给这个`Socket`赋一个`IP`和`端口`。原因如下:
* `IP`: 一台机器可能会有多个网卡，也就有多个`IP`地址,可以选择监听所有的网卡，也可以选择监听一个，只有发给这个网卡的包才会给你。
* `端口`: 一个网络包来的时候，内核要通过`TCP头`里的这个端口，来找到你这个应用程序，把包给你。

当服务器有`IP`和`端口号`, 就可以调用`listen`函数进行监听。在`TCP`的状态图里，有个`listen`状态，当调用这个函数之后， 服务器就进入这个状态， 客户端就可以发起连接。

内核中为每个`Socket`维护两个队列：
* 已经建立连接的队列， 这时连接三次握手已经完毕，处于`established`状态
* 还没有完全建立连接的队列，这时三次握手还没有完成，处于`syn_rcvd`状态

接下来，服务端调用`accept`函数，拿出一个已完成的连接进行处理，如还没完成就要等着。在服务端等待的时候，客户端可以通过`connect`函数发起连接，先在参数中指明要连接的`IP地址`和`端口号`， 然后开始发起三次握手，内核会给客户端分配一个临时的端口。一旦握手成功，服务端的`accept`就会返回另一个`Socket`。监听的`Socket`和真正用来传数据的`Socket`是两个，一个叫做**监听Socket**, 一个叫做**已连接Socket**。连接建立成功之后，双方开始通过`read`和`write`函数来读写数据，就像往一个文件流里写东西一样。

![Socket](http://7xpgi9.com1.z0.glb.clouddn.com/socket1.JPG "Socket")

在内核中`Socket`是一个文件，对应就有文件描述符。每个进程都有一个数据结构`task_struct`，里面指向一个文件描述符数组，来列出这个进程打开的所有文件的文件描述符。文件描述符是个整数，是这个数组的下标。这个数组中的内容是一个指针，指向内核中所有打开的文件列表。既然是一个文件，就会有一个`inode`, 只不过`Socket`对应的`inode`不像真正的文件系统一样，保存在硬盘上，而是在内存中。这个`inode`指向`Socket`在内核中的`Socket`的结构。

这个结构里，主要是两个队列，1.**发送队列** 2.**接收队列**。在这两个队列里保存的是一个缓存`sk_buff`。这个缓存里能够看到完整的包结构。

### 基于UDP协议的Socket程序函数调用过程
`UDP`是没有连接的，不需要三次握手，也就不需要调用`listen`和`connect`, 但`UDP`交互仍需要`IP`和`端口号`就需要`bind`。`UDP`是没有维护连接状态的，因而不需要每对连接建立一组`Socket`, 而是只要有一个`Socket`就能和多个客户端通信。也正是因为没有连接状态，每次通信时，都调用`sendto`和`recvfrom`, 都可以传入`IP地址`和端口。

### 服务器如何连接更多项目
**最大连接数**, 系统会用一个四元祖来标识一个`TCP`连接。
> {本机IP, 本机端口，对端IP, 对端端口}

服务器通常固定在某个本地端口上监听，等待客户端的连接请求。因此，服务端`TCP`连接四元祖中只有`对端IP`（客户端`IP`）和 `对端端口`(客户端端口)是可变的。因此:
> 最大`TCP`连接数 = 客户端`IP`数 x 客户端端口数

> 对于`IPv4`，客户端`IP`数最多为`2^32`, 客户端端口数最多为`2^16`, 也就是服务端单机最大`TCP`连接数，约为 `2^48`。

当然，服务端最大并发`TCP`连接数远不能达到理论上限。首先主要是**文件描述符限制**, 按照上面的原理，`Socket`都是文件，所以首先要通过`ulimit`配置文件描述符数目；另一个限制是**内存**, 按照上面的数据结构，每个`TCP`连接都要占用一定内存，操作系统内存是有限的。

可以通过以下方式来降低每个项目消耗的资源数目：

![方式一](http://7xpgi9.com1.z0.glb.clouddn.com/socket2.JPG "方式一")

* **将项目外包给其他公司(多进程方式)**: 一旦建立一个连接，就会有个已连接`Socket`, 这时可以创建一个子进程，然后将基于已连接`Socket`的交互交给这个新的子进程来做。在`Linux`下，创建子进程使用`fork`函数（在父进程的基础上完全拷贝一个子进程）。在`Linux`内核中，会复制文件描述符的列表，也会复制内存空间，还会复制一条记录当前执行到哪行程序的进程。显然, 复制的时候调用`fork`, 复制完毕之后，父进程和子进程都会记录当前刚刚执行完`fork`。这两个进程刚复制完的时候，几乎一摸一样，只是根据`fork`的返回值来区分到底是父进程(返回其他整数)，还是子进程(返回`0`)。因为复制了文件描述符列表，而文件描述符都是指向整个内核统一的打开文件列表的，因而父进程刚才因为`accept`创建的已连接`Socket`也是一个文件描述符，同样会被子进程获得。 接下来, 子进程可以通过这个已连接`Socket`和客户端进行互通了，当通信完毕之后，就可以退出进程。（`fork`时返回整数就是父进程，这个整数就是子进程的`ID`, 父进程可通过这个`ID`查看子进程是否完成项目，是否需要退出）

![方式二](http://7xpgi9.com1.z0.glb.clouddn.com/socket3.JPG "方式二")

* **将项目转包给独立的项目组(多线程方式)**: 相比`进程`来说，创建`线程`就相当于在同一个公司成立项目组。一个项目做完了，那这个项目组就可以解散，组成另外的项目组，办公家具都可公用。在`Linux`下，通过`pthread_create`创建一个线程，也是调用`do_fork`。 不同的是，虽然新的线程在`task`列表会新创建一项，但是很多资源，例如**文件描述符列表**，**进程空间**，还是共享的，只不过多了一个应用而已。新的线程也可以通过已连接`Socket`处理请求，从而达到并发处理的目的。

> 上面基于进程或线程模型，其实还是有问题的。新到来一个`TCP`连接，就需要分配一个进程或线程。一台机器无法创建很多进程或线程。有个`C10K`(**一台机器要维护`1万`个连接，就要创建`1万`个进程或线程，操作系统是无法承受的**)。

* **一个项目组支撑多个项目(IO多路复用，一个线程维护多个Socket)**: 由于`Socket`是文件描述符，因而某个线程盯的所有`Socket`,都放在一个文件描述符集合`fd_set`中，这就是**项目进度墙**，然后调用`select`函数来监听文件描述符集合是否有变化。一旦变化，就会依次查看每个文件描述符。那些发生变化的文件描述符在`fd_set`对应的位都设为`1`，表示`Socket`可读可写，从而可以进行读写操作，然后调用`select`进行下一轮。

![方式四](http://7xpgi9.com1.z0.glb.clouddn.com/socket4.JPG "方式四")

* **一个项目组支撑多个项目(IO多路复用，从"派人盯着"到"有事通知")**: 上面`select`函数还是有问题，因为每次`Socket`所在地恶文件描述符集合中`Socket`发生变化时，都需要通过轮询的方式来查看进度，这大大影响了一个项目组能支撑的最大项目数量。因而使用`select`, 能够同时盯的项目数量由`FD_SETSIZE`限制。如果改成事件通知的方式，情况就会好很多，项目组不需要通过轮询挨个盯着这些项目，而是当项目进度发生变化时，主动通知项目组，然后项目组再根据项目进展情况做相应的操作。能完成这件事的函数叫`epoll`, 它在内核中的实现不是通过轮询的方式，而是通过注册`callback函数`的方式，当某个文件描述符发送变化时，就会主动通知。如图所示，假设进程打开了`Socket` `m n x`等多个文件描述符，现在需要通过`epoll`来监听是否这些`Socket`都有事件发生。其中`epoll_create`创建一个`epoll对象`，也是一个文件，也对应一个文件描述符，同样对应着打开文件列表中的一项。在这项里有一个红黑树，在红黑树里，要保存这个`epoll`要监听所有`Socket`。当`epoll_ctl`添加一个`Socket`时，其实是加入这个红黑树，同时红黑树里面的节点指向一个结构，将这个结构挂在被监听的`Socket`事件列表中。到一个`Socket`来了一个事件时，可以从这个列表中得到`epoll对象`,并调用`callback`通知它。这种通知方式使得监听的`Socket`数据增加的时候，效率不会大幅度降低，能够同时监听的`Socket`的数目也非常多。上限为系统定义的，进程打开的最大文件描述符个数。因而，**epoll被称为解决`C10K`问题的利器**。

## HTTP协议
登陆 [www.163.com](http://www.163.com) 这个 `URL`(同一资源定位符), 浏览器会将 [www.163.com](http://www.163.com) 这个域名发送给 `DNS服务器`, 让他解析`IP地址`. `HTTP` 先通过三次握手建立 `TCP连接`. (目前使用的`HTTP协议`大部分都是`1.1`, 是默认开启 `Keep-Alive`, 建立的`TCP`连接，可以在多次请求中复用)。

`HTTP`的报文可以分为`3`大部分
* 请求行
请求行中`URL`就是[www.163.com](http://www.163.com)，版本为`HTTP 1.1`, **方法**有几种类型:
    * 访问网页来说，最常用的就是`GET`，服务器决定放回什么格式的内容(`JSON`字符串等)。
    * `POST` 需要主动告诉服务器一些信息。一般会放在正文里。正文可以用各种各样的格式。常见的格式也是`JSON`.
    * `PUT` 向指定资源位置上传最新内容. 但是，`HTTP`服务器往往不允许上传文件， 所以 `PUT` 和 `POST` 都变成要穿东西给服务器的方法。实际使用过程中， `POST` 往往用来创建一个资源，`PUT` 用来修改资源。
    * `DELETE` 用来删除资源。

* 首部
首部都是 `key` `value` 格式，通过冒号分隔。 里面保存一些非常重要的字段。例如:
    * `Accept-Charset` 表示客户端可以接收的字符集, 防止产生乱码。
    * `Content-Type` 表示正文的格式，我们使用 `POST` 请求， 如果正文是`JSON`这个值设置为`JSON`.
    * `Cache-control` 用来控制缓存
    * `If-Modified-Since`
* 正文实体

> `HTTP`请求发送: `HTTP协议` 是基于`TCP协议`的， 使用面向连接的方式发送请求， 通过 `stream` 二进制流的方式传给对方， 到了`TCP层`, 会把二进制流变成一个个报文段发送给服务器.

### HTTP返回的构建
`HTTP` 返回报文也有一定格式， 也是基于`HTTP 1.1`.
* 状态行
    * 状态码： 会反应`HTTP`请求的结果. `200`意味一切顺利; `404` 表示服务器无法响应这个请求。
    * 短语: 会大概说明一下原因。
* 首部
    * `Retry-After`表示告诉客户端应该在多长时间之后再尝试。`503`错误表示**服务器暂时不再和这个值配合使用**。
    * `Content-Type` 表示返回的是`HTML` 还是 `JSON`.

> `HTTP 1.1` 在应用层以纯文本的形式进行通信。 每次通信都要带完整的`HTTP头`, 不考虑`pipeline模式`的话， 每次的过程总是一来一回， 在**实时性**, **并发性**上都存在问题。 `HTTP 2.0` 会对 `HTTP头`进行一定的压缩， 将原来每次都要携带得的大量的 `key value` 在两端建立一个索引表, 对相同的头只发送索引表中的索引。

`Google` 的 `QUIC协议`, 从 `TCP` 切换到 `UDP`, 有如下几个特点:
* 自定义连接机制
> 不再以四元组(`源IP`,`源端口`,`目的IP`,`目的端口`)标识，而是以一个`64位`随机数作为`ID`来标识，而且`UDP`是无连接的，所以`IP`或`端口`变化时, 只要`ID`不变， 就不需要重新建立连接.

* 自定义重传机制
> `TCP` 为了保证可靠性，通过使用**序号**和**应答**机制类解决顺序和丢包问题。`QUIC`定义来一个`offset`的概念。`QUIC`是面向连接的，就像`TCP`一样是一个数据流, 发送的数据在这个数据流里有个偏移量`offset`, 可以通过 `offset` 查看数据发送到哪里， 只要`offset`包没有来就要重发。

* 无阻塞多路复用
> 有了自定义的连接和重传机制就可以解决`HTTP 2.0`的多路复用问题。

* 自定义流量控制
> `TCP`通过**滑动窗口协议**来实现流量控制。`QUIC`也通过`window_update`, 但是是适应自己的多路复用机制的， 不但在一个连接上控制窗口， 还在一个连接中的每个`steam`控制窗口。

## HTTPS协议
加密分为两种方式：
* **对称加密**
> 对称加密算法中，加密和解密使用的密钥是相同的。因此，堆成加密算法要保证安全性的话，密钥要做好保密。只能让使用的人知道，不能对外公开。                    

* **非对称加密**
> 加密使用的密钥和解密使用的密钥是不相同的。一把是作为公开的公钥，另一把是作为谁都不能给的私钥。公钥加密的信息，只能私钥才能解密。私钥加密的信息，只有公钥才能解密。客户端也需要有自己的公钥和私钥。

所以对称加密算法相比非堆成加密算法来说，效率高很多，性能也好，所以交互的场景下多用对称加密。

### 数字证书
不对称加密也会有同样的问题，何如将不对称加密的公钥给对方？一种是放在一个公网的地址上，让对方下载；另一种是在建立连接时传给对方。两种方法有相同的问题，作为一个普通的网民，怎么鉴别公钥是对的，例如，我自己搭建一个网站`cliu8site`可以通过以下命令先创建私钥：

```plain
openssl genrsa -out cliu8siteprivate.key 1024
```

然后再根据私钥创建对应的公钥。

```plain
openssl rsa -in cliu8siteprivate.key -pubout -outcliu8sitepublic.pem
```

这个时候就需要权威部门介入，就像每个人都可以打印自己的简历，说自己是谁，但有公安局盖章的，就只有户口本，才能证明你是你。这个由权威部门颁发的称为**证书(Certificate)**。

**证书(Certificate)**里应该有**公钥**, 还有证书的**所有者**,就像户口本上有你的姓名和身份证号，说明这个户口本是你的；另外还有证书的**发布机构**和证书的**有效期**。这个证书是怎么生成的呢? 会不会有人假冒权威机构颁发证书？就像有假身份证，假户口一样。生成证书需要发起一个证书请求，然后将这个请求发给一个权威机构去认证，这个权威机构我们称为**CA(`Certifcate Authority`)**, 证书请求可以通过命令生成：

```plain
openssl req -key cliu8siteprivate.key -new -out cliu8sitecertificate.req
```

将这个请求发给权威机构，权威机构会给这个证书盖章，我们称为**签名算法**. 只有使用 `CA`的私钥签名才能保证是真的权威机构的签名。签名算法大概的工作流程：对信息做一个`Hash`计算，得到一个`Hash`值，这个过程是不可逆的，再把信息发送出去时，把这个`Hash`值加密后，作为一个签名和信息一起发出去。权威机构给证书签名的命令如下:

```plain
openssl x509 -req -in cliu8sitecertificate.req -CA cacertificate.pem -CAkey \
caprivate.key -out cliu8sitecertificate.pem
```

这个命令会返回`Signature ok`, 而`cliu8sitecertificate.pem`就是签过名的证书。`CA`用自己的私钥给外卖网站的公钥签名，就相当于给外卖网站背书，形成外卖网站的证书。证书的内容：

```plain
openssl x509 -in cliu8sitecertificate.pem -noout -text
```

这里有个`Issuer`(证书是谁颁发的)；`Subject`(证书颁发给谁); `Validity`(证书期限)；`Public-key`(公钥内容);`Signature Algorithm`(签名算法)。这下你不会从外卖网站上得到公钥，而是得到一个证书，这个证书有个发布机构`CA`,你只要得到这个发布机构`CA`的公钥，去解密外卖网站证书的签名，如果解密成功，`Hash`也对上，就说明这个外卖网站的公钥没啥问题。

要想验证证书，需要`CA`的公钥，问题是，怎么确定`CA`的公钥是对的？所以，`CA`的公钥需要更牛的`CA`给它签名，然后形成`CA`的证书。要想知道某个`CA`的证书是否可靠，要看`CA`的上级证书是否可靠，要看`CA`的上级证书的公钥，能不能解开这个`CA`的签名，`CA`的公钥也需要更牛的`CA`给它签名，然后形成`CA`的证书，这样层层上去，直到全球皆知的几个著名大`CA`,称为`root CA`, 做最后的背书。通过这种 **层层授信背书**的方式，从而保证了非对称加密模式的正常运转。

除此之外，还有一种证书，称为**Self Signed Certificate**, 给人一种"我就是我，你爱信不信"。

### HTTPS的工作模式
我们知道，非对称加密在性能上不如对称加密，那是否能将两者结合？例如，公钥私钥主要用于传输对称加密的秘钥，而真正的双方大数据量的通信都是通过对称加密进行的。这就是`HTTPS`协议的总体思路:
![Https](http://7xpgi9.com1.z0.glb.clouddn.com/https1.JPG "Https")

当你登陆外卖网站时，由于是`HTTPS`,客户端会发送`Client Hello`消息到服务器，以明文传输`TLS`版本信息，加密套件候选列表，压缩算法候选列表等信息。另外，还会有一个随机数，在协商对称秘钥时使用。然后，外卖网站返回`Server Hello`消息告诉客户端，服务器选择使用的协议版本，加密套件，压缩算法等，还有一个随机数，用于后续的密钥协商。然后外卖网站会给一个服务端的证书说："Server Hello Done, 我这里就这些信息了"。

你当然不相信这个证书，于是从自己信任的`CA`仓库中，拿`CA`的证书里面的公钥去解密外卖网站的证书。如果能够成功，说明外卖网站是可信的。这个过程中，可能会不断往上追溯`CA`, `CA`的`CA`, `CA`的`CA`的`CA`,反正直到一个授信的`CA`。

证书验证完毕之后，觉得这个外卖网站可信，于是客户端计算产生随机数字`Pre-master`, 发送`Client Key Exchange`, 用证书中的公钥加密，再发送给服务器，服务器可以通过私钥解密出来。

到目前为止，无论是客户端还是服务器，都有三个随机数，分别是：自己的，对端的，以及刚生成的`Pre-master`随机数。通过这三个随机数，可以在客户端和服务器产生相同的对称密钥。有了对称密钥，客户端就可以说："`Change Clipher Spec`", 咱们以后都采用协商的通信密钥和加密算法进行加密通信。然后发送一个`Encrypted Handshake Message`, 将已经商定好的参数等，采用协商密钥进行加密，发送给服务器用于数据与握手验证。同样，服务器也可以发送`Change Cipher Spec`, 说:"没问题，咱们以后都采用协商的通信密钥和加密算法进行加密通信",并且也发送`Encrypted Handshake Message`的消息试试。当双方握手结束之后，就可以通过对称密钥进行加密传输。这个过程除了加密解密外，其他的过程和`HTTP`是一样的，过程也非常复制。

上面的过程只包含`HTTPS`的单向认证，也即客户端验证服务端的证书，是大部分的场景，也可以在更加严格安全要求的情况下，启用双向认证，双方互相验证证书。

### 重放与篡改
有了加密和解密, 黑客截获包也打不开，但它可以发送`N`次。这个往往通过`Timestamp`和`Nonce`随机数联合起来，然后做一个不可逆的签名来保证。`Nonce`随机数保证唯一，或者`Timestamp`和`Nonce`合起来保证唯一，同样的，请求只接收一次，于是服务器多次受到相同的`Timestamp`和`Nonce`, 则视为无效即可。如果有人想篡改`Timestamp`和`Nonce`, 还有签名保证不可篡改性，如果改了用签名算法解出来，就对不上了，可以丢弃。

## 流媒体协议
无论是直播还是点播，其实都是对于视频数据的传输。视频技术的三个名词系列：
* **名词系列一**： `AVI`, `MPEG`, `RMVB`, `MP4`, `MOV`, `FLV`, `WebM`, `WMV`, `ASF`, `MKV`。例如`RMVB`和`MP4`是不是很熟悉？
* **名词系列二**：`H.261`, `H.262`, `H.263`, `H.264`, `H.265`。这里重点关注 `H.264`。
* **名词系列三**：`MPEG-1`, `MPEG-2`, `MPEG-4`, `MPEG-7`, `MPEG`听说过，但是后面的数字是怎么回事?

视屏其实就是快速播放的一连串连续的图片。每一张图片，我们称为一**帧**。只要每秒钟帧的数据足够多，也即播放的足够快。比如每秒`30`帧，以人眼的敏感程度，是看不出这是一张张独立的图片的，这就是我们常说的**帧率**。每一张图片都是由像素组成的，假设为`1024x768`(这个像素不算多)。每个像素由`RGB`组成，每个`8`位，共`24`位。

我们来算一下，每秒钟的视频有多大？
> `30`帧 x `1024` x `768` x `24` = `566,231,040`(Bits) = `70,778,880`(Bytes)

> 如果是一分钟就是`4,246,732,800Bytes`, 已经是`4G`。这个数据量实在太大，根本没办法存储和传输。如果这样存储，硬盘很快就满了。这样传输，多少带宽也不够用。可以使用**编码**来用尽量上的`Bit`数保存视频，使播放时画面看起来仍然很精美。**编码是一个压缩的过程**。

**视屏和图片压缩过程中的特点**:
* **空间冗余**: 图像相邻像素之间有较强的相关性，一张图片相邻像素往往是渐变的，不是突变的，没必要每个像素都完整的保存，可以隔几个保存一个，中间用算法计算出来。
* **时间冗余**: 视频序列的相邻图像之间内容相似。一个视频中连续出现的图片也不是突变的，可以根据已有图片进行预测和推断。
* **视觉冗余**: 人的视觉系统对某些细节不敏感，因此不会每一个细节都注意到，可以允许丢失一些数据。
* **编码冗余**: 不同像素值出现的概率不同，概率高的用的字节少，概率低的用的字节多，类似**[霍夫曼编码](https://zh.wikipedia.org/wiki/%E9%9C%8D%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81)(Huffman Coding)**思路。

![视频压缩](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3783.JPG "视频压缩")

总之，用于编码的算法非常复杂，而且多种多样，但是编码过程其实都是类似的。

**视频编码两大流派**:
* **流派一**: `ITU`(International Telecommunications Union)的`VCEG`(Video Coding Experts Group), 这个称为**国际电联下的VCEG**。既然是电信，他们最初做视频编码，主要侧重传输。名词**系列二**，就是这个组织定的标准。
* **流派二**: `ISO`(International Standards Organization)的`MPEG`(Moving Picture Experts Group), 这个是**ISO旗下的MPEG**, 本来是做视频存储的。例如，编码后保存在`VCD`和`DVD`中。当然后来也慢慢侧重视频传输了。名词**系列三**就是这个组织制定的标准。

后来，`ITU-T`(国际电信联盟电信标准化部门，ITU Telecommunication Standardization Sector) 与 `MPEG`联合制定了`H.264/MPEG-4 AVC`,这才是我们这里关注的重点。进过编码之后，一帧帧的图像就变成一串串**二进制**,这个**二进制**可以放在一个文件里，按照一定的格式保存起来，这就是**名词系列一**。其实这些就是视频保存成文件的格式。例如，前几个字节是什么意义，后几个字节是什么意义，然后是数据，数据中保存的就是编码好的结果。

![直播流程](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3784_1.JPG "直播流程")

当然，这个**二进制**也可以通过某种网络协议进行封装，放在互联网上传输，这个时候就可以进行网络直播了。网络协议将**编码**好的视频流，从主播端推送到服务器，在服务器上有个运行了同样协议的服务端来接收这些网络包，从而得到里面的视频流，这个过程称为**接流**。

服务端接到视频流之后，可以对视频流进行一定的处理，例如**转码**(从一个编码格式，转成另一种格式)。因为观众使用的客户端千差万别，要保证他们都能看到直播。**流处理**完毕之后，就可以等待观众的客户端来请求这些视频流。观众的客户端请求的过程称为**拉流**。

如果有非常多的观众，同时看一个视频直播，那都从一个服务器上**拉流**，压力太大，因而需要一个视频的**分发网络**，将视频预先加载到就近的边缘节点，这样大部分观众看的视频，是从边缘节点拉取的，就能降低服务器的压力。

当观众的客户端将视频流拉下来之后，就需要进行**解码**, 也即通过上述过程的逆过程，将一串串看不懂的二进制再转变成一帧帧生动的图片，在客户端**播放**出来。

接下来，我们依次看下每个过程：

![直播流程](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3785.JPG "直播流程")

* **编码：如何将丰富多彩的图片变成二进制流 **
> 虽然我们说视频是一张张图片的序列，但如果每张图片都完整，就太大了，因而会将视频序列分成**三种帧**:
* **I帧**: 也称关键帧。里面是完整的图片，只需要本帧数据，就可以完成解码
* **P帧**: 前向预测编码帧。`P帧`表示的是这一帧跟之前的一个关键帧(或`P帧`)的差别，解码时需要用之前缓存的画面，叠加上和本帧定义的差别，生成最终画面。
* **B帧**: 双向预测内插编码帧。`B帧`记录的是本帧与前后帧的差别。要解码`B帧`,不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的数据与本帧数据的叠加，取得最终的画面。

> 可以看出，`I帧`最完整，`B帧`压缩率最高，而压缩后帧的序列，应该在`IBBP`的间隔出现的。这就是**通过时序进行编码**。在一帧中，分成多个片，每个片中分成多个宏块，每个宏块分成多个子块，这样将一张大的图分解成一个个小块, 方便进行**空间上的编码**。, 

> 尽管时空非常立体的组成了一个序列，但是总归还是要压缩成一个二进制流。这个流是有结构的，是一个个**网络提取层单元(NALU, Network Abstraction Layer Unit)**。变成这种格式就是为了传输，因为网络上的传输，默认的是一个个的包，因而这里也就分成了一个个单元。

![NALU](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3786.JPG "NALU")

每个`NALU`首先是一个起始标识符，用于表示`NALU`之间的间隔；然后是`NALU`的头，里面主要配置`NALU`的类型；最终`Payload`里面是`NALU`承载的数据。在`NALU`头里，主要内的是容类型`NAL Type`:
* `0x07`表示`SPS`, 是序列参数集，包括一个图像序列的所有信息，如图像尺寸，视频格式等。
* `0x08`表示`PPS`, 是图像参数集，包括一个图像的所有分片的所有相关信息，包括图像类型，序列号等。

> 在传输视频流之前，必须要传输这两类参数，不然无法解码。为了保证容错性，每个`I帧`前面，都会传一遍这两个参数集合。**如果`NALU Header`里面的表示类型是`SPS`或`PPS`**, 则`Payload`中就是真正的参数集内容。**如果类型是帧**，则`Payload`中才是真的视频数据，当然也是一帧帧存放的，前面说了，一帧的内容还是挺多的，因而每一个`NALU`里保存的是一片。对于每一片，到底是`I帧`, 还是`P帧`,还是`B帧`,在片结构里也有个`Header`, 里面有个类型， 然后是片的内容。

> 这样，整个格式就出来了，**一个视频，可以拆分成一系列的帧，每帧拆分成一系列的片，每片都放在一个`NALU`里面，`NALU`之间都是通过特殊的起始标识符分隔，在每个`I帧`的第一篇前面，要插入单独保存`SPS`和`PPS`的`NALU`,最终形成一个长长的`NALU`序列**。

* **推流: 如何把数据流打包传输到对端**
这个格式还是不能直接在网上传输到对端的，还需要将这个二进制流打包成网络包进行发送，这里我们使用`RTMP协议`。这就进入第二个流程**推流**。`RTMP`是基于`TCP`的，因而肯定需要双方建议一个`TCP`连接。在有`TCP`连接的基础上，还需要建立一个`RTMP`连接，也即在程序里面，需要调用`RTMP`类库的`Connect`函数显示创建一个连接。

> `RTMP`为啥需要建立一个单独的连接？因为它们需要商量一些事情，保证以后传输能正常进行，主要就是两个事情:`1.`**版本号**，如果客户端服务器的版本号不一致，则不能工作。`2.`**时间戳**，视频播放中，时间是很重要的，后面的数据流互通时，经常要带上时间戳的差值，因而一开始双方就要知道对方的时间戳。 

未来沟通这些事情，需要发送六条消息；客户端发送`C0`,`C1`,`C2`, 服务器发送`S0`,`S1`,`S2`。首先客户端发送`C0`表示自己的版本号，不必等对方的回复，然后发送`C1`表示自己的时间戳。服务器只有在收到`C0`时，才能返回`S0`,表明自己的版本号，如果版本号不匹配，可以断开连接。服务器发送完`S0`后，也不用等什么，就直接发送自己的时间戳`S1`。客户端收到`S1`时，发一个知道对方时间戳的`ACK C2`。同理服务器收到`C1`时，发送一个知道对方时间戳的`ACK S2`。

![RTMP](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3787.JPG "RTMP")

握手之后，双方需要互相传递一些控制信息，例如`Chunk`块的大小，窗口大小等；真正传输时，还需要创建一个流`Stream`, 然后通过这个`Stream`来推流`publish`。**推流**的过程就是将`NALU`放在`Message`里面发送，这个也称为`RTMP Packet包`。`Message`的格式如下：

![Message](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3788_1.JPG "Message")

发送时去掉`NALU`起始标识符，因为这部分对于`RTMP`协议来说没有用。接下来，将`SPS`和`PPS`参数集封装成一个`RTMP`包发送，然后发送一个个片的`NALU`。`RTMP`在收发数据时并不是以`Message`为单位的，而是把`Message`拆分为`Chunk`发送，而且必须在一个`Chunk`发送完成之后，才能开始发送下一个`Chunk`.每个`Chunk`中都带有`Message ID`, 表示属于哪个`Message`, 接收端也会按照这个`ID`将`Chunk`组成`Message`。前面连接时，设置的`Chunk`块大小就是指这个`Chunk`，将大的消息变为小的块再发送，可以在低带宽的情况下，减少网络拥塞。如下是一个分块的例子:

> 假设一个视频的消息长度为`307`, 但是`Chunk`大小约定为`128`,于是会拆分为三个`Chunk`。第一个`Chunk`的`Type = 0`, 表示`Chunk`头是完整的；头里`Timestamp`为`1000`,总长度`Length`为`307`, 类型为`9`, 是个视频，`Stream ID`为`12346`, 正文部分承担`128`字节的`Data`。第二个`Chunk`也要发送`128`个字节，`Chunk`头由于和第一个`Chunk`一样，因此采用`Chunk Type = 3`,表示头一样就不再发送。第三个`Chunk`要发送的`Data`的长度为`307-128-128 = 51`个字节，还是采用`Type = 3`。

![Message](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3789_1.JPG "Message")

就这样数据就源源不断到达流媒体服务器，整个过程就像这样。

![Message](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3790.JPG "Message")

这时，大量观看直播的观众就可以通过`RTMP`协议从流媒体服务器上拉取，但这么多的用户量，都去同一个地方拉取，服务器压力会很大，而且用户分布在全国甚至全球，如果都去统一的一个地方下载，也会时延比较长，需要有分发网络。分发网络分为**中心**和**边缘**两层。边缘层服务器部署在全国各地及横跨各大运营商里，和用户距离很近。中心层是流媒体服务集群，负责内容的转发。智能负载均衡系统，根据用户的地理位置信息，就近选择边缘服务器，为用户提供 **推/拉** 流服务。中心层也负责转码服务，例如，把`RMTP`协议的码流转换为`HLS`码流。

![Message](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3791.JPG "Message")

这套机制在后面的`DNS`, `HTTPDNS`, `CDN`会详细描述。

* **拉流: 观众的客户端如何看视频**

![Message](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3792.JPG "Message")

先读到的是`H.264`的解码参数，例如`SPS`和`PPS`，然后对收到的`NALU`组成一个个帧，进行解码，交给播放器播放，一个绚丽多彩的视频画面就出来了。

## P2P 协议
下载一部电影，最简单的方式就是通过`HTTP`进行下载，但是通过浏览器下载时，只要文件稍微大点，下载速度就奇慢无比。还有种下载文件的方式，就是通过`FTP`(文件传输协议), `FTP`采用两个`TCP`连接来传输一个文件。
* **控制连接**: 服务器以被动的方式，打开众所周知用于`FTP`的端口(`21`), 客户端则主动发起连接。该连接将命令从客户端传给服务器，并传回服务器的应答。常用的命令有: `list`(获取文件目录), `reter`(取一个文件), `store`(存一个文件)。
* **数据连接**: 每当一个文件在客户端与服务器之间传输时，就创建一个数据连接。

### FTP 的两种工作方式
每传输一个文件，都要建立一个全新的数据连接。`FTP`有两种工作模式，分为`主动模式(PORT)`和`被动模式(PASV)`，这些都是站在`FTP`服务器的角度来说的：
* **主动模式**
客户端随机打开一个大于`1024`的端口`N`, 向服务器的命令端口`21`发起连接，同时开放`N+1`端口监听，并向服务器发出`port N+1`命令，由服务器从自己的数据端口`20`, 主动连接到客户端指定的数据端口`N+1`

* **被动模式**
当开启一个`FTP`连接时，客户端打开两个任意的本地端口(大于`1024`)和`N+1`.第一端口连接服务器的`21`端口，提交`PASV`命令。然后，服务器会开启一个任意的端口`P`(大于`1024`),返回"227 entering passive mode"消息，里面有`FTP`服务器开放的用来进行数据传输的端口。客户端收到消息取得端口号之后，会通过`N+1`号端口连接服务器端口`P`, 然后在两个端口之间进行数据传输。

### P2P是什么？
无论是`HTTP`方式，还是`FTP`方式，都有一个比较大的缺点，就是**难以解决单一服务器的带宽压力**，因为它们使用的都是传统的客户端服务器的方式。 后来，一种创新的，称为`P2P`的方式流行起来。`P2P`(peer-to-peer)资源开始并不集中地存储在某些设备上，而是分散地存储在多台设备上。这些设备我们称为`peer`。想要下载一个文件时，只要得到那些已经存在文件的`peer`,并和这些`peer`之间，建立点对点的连接，而不需要到中心服务器上，就可以就近下载文件。一旦下载了一个文件，你也会称为`peer`中的一员，你旁边的那些机器，也可能会选择从你这里下载文件，所以当你使用`P2P`软件时，例如**BitTorrent**, 往往能够看到，既有下载流量，也有上传流量，也即你自己也加入了这个`P2P`网络，自己从别人那里下载，同时也提供给其他人下载。这种方式，参与的人越多，下载速度越快。

### 种子(.torrent)文件
要想知道哪些`peer`有这些文件，需要用到种子(`.torrent文件`), `.torrent文件`由两部分组成:
* **announce (tracker URL)**
* **文件信息**
    * **info区**: 指定该种子有几个文件，文件长度，目录结构，以及目录和文件名称
    * **Name字段**: 指定顶层目录名字
    * **每个段的大小**: `BitTorrent(简称BT)`协议把一个文件分成很多个小段，然后分段下载
    * **段哈希值**: 将这个种子中，每个段的`SHA-1`哈希值拼在一起。

> 下载时，`BT`客户端首先解析`.torrent`文件，得到`tracker`地址，然后连接`tracker`服务器。`tracker`服务器回应下载者的请求，将其他下载者(包括发布者)的`IP`提供给下载者。下载者再连接其他下载者，根据`.torrent`文件，两者分别告知对方自己已经有的块，然后交换对方没有的数据。此时不需要其他服务器参与，并分散了单个线路上的数据流量，减轻了服务器负担。

> 下载者每得到一个块, 需要算出下载块的`Hash`验证码，并与`.torrent`文件中的对比。如果一样，则说明块正确，不一样则需重新下载这个块。这规定是为了解决下载内容的准确性问题。从这个过程也可以看出，这种方式特别依赖`tracker`。`tracker`需要收集下载者信息的服务器，并将此信息提供给其他下载者，使下载者们相互连接起来传输数据。虽然下载的过程是非中心化的，但加入这个`P2P`网络时，都需要借助`tracker`中心服务器，这个服务器是用来登记有哪些用户在请求哪些资源。这种工作方式有一个弊端，一旦`tracker`服务器出现故障或线路遭到屏蔽，`BT`工具就无法正常工作。

### 去中心化网络(DHT)
**DHT(Distributed Hash Table)**: 每个加入这个`DHT`网络的人，都要负责存储这个网络里的资源信息和其他成员的联系信息，相当于所有人一起构成一个庞大的分布式存储数据库。**Kademlia协议**是一种著名的`DHT协议`:

> 任何一个`BitTorrent`启动之后，都有两个角色。一是`peer`,监听一个`TCP`端口，用来上传和下载文件，这个角色表明，这里有某个文件。另外的角色是`DHT node`, 监听一个`UDP`端口，通过这个角色，这个节点加入一个`DHT`网络。

> 在`DHT`网络里，每一个`DHT node`都有一个`ID`, 这个`ID`是一个很长的串。每个`DHT node`都有责任掌握一些知识，也就是**文件索引**, 就是它应该知道某些文件是保存在哪些节点上。只需要有这些知识就可以了，而它自己本身不一定保存这个文件节点。

![DHT](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3820.JPG "DHT")

### 哈希值
每个`DHT node`不会有全局知识(不知道所有文件保存在哪里), 只需要知道一部分。应该知道哪一部分需要用哈希值计算出来，**DHT node的ID是和哈希值相同长度的串**。`DHT`算法是这样规定的：如果一个文件计算出一个哈希值，则和这个哈希值一样的那个`DHT node`, 就有责任知道从哪里下载这个文件，即便它自己没有保存这个文件。可能一摸一样的`DHT node`下线了，所以`DHT`算法还规定：除了一摸一样的那个`DHT node`应该知道，`ID`和这个哈希值非常接近的`N`个`DHT node`也应该知道。

> **哈希值接近的定义:** 例如只修改最后一位，就很接近；修改倒数`2`位也不远; 修改倒数`3`位也可以接收。总之，凑齐规定的`N`个数就行。

分析上图, `文件1`通过哈希运算，得到匹配`ID`的`DHT node`为`node C`, 当然还会有其他的。所以`node C`有责任知道`文件1`的存放地址，虽然`node C`本身没有存放`文件1`。同理，`文件2`通过哈希运算，得到匹配`ID`的`DHT node`为`node E`，但是`node D`和`node E`的`ID`值很近, 所以`node D`也知道。当然，`文件2`本身没有必要一定在`node D`和`node E`里，但是碰巧在`node E`里有一份。

接下来一个新的节点`node new`上线了。如果想下载`文件1`，首先要加入`DHT`网络；
> 这种模式下，种子`.torrent文件`里就不再是`tracker`地址，而是一个`list`的`node`地址，而所有这些`node`都是已经在`DHT`网络里的。随着时间的推移, 很可能有退出，下线，但我们假设，不会所有的都联系不上，`node new`只要在种子里找到一个`DHT node`就加入网络。`node new`会计算`文件1`的哈希值，并根据这个哈希值了解到和这个哈希值匹配或很接近的`node`上知道如何下载这个文件，例如计算出的哈希值就是`node C`。

> 但是`node new`不知道怎么联系上`node C`, 因为种子里的`node`列表里很可能没有`node C`, 但是它可以问，`DHT`网络特别像一个社交网络，`node new`只要去它能联系上的`node`问。在`DHT`网络中，每个`node`都保存一定的联系方式，但肯定没有`node`的所有联系方式。`DHT`网络中，节点之间通过互相通信，也会交流联系方式，也会删除联系方式。有个理论是，社交网络中，任何两个人直接的距离不超过`6`。所以，`node new`想联系`node C`, 就去万能的朋友圈问。如果找不到`node C`, 也能找到和`node C`的`ID`很像的节点，它们也知道如何下载`文件1`。

>在`node C`上，告诉`node new`, 下载`文件1`，要去`B`, `D`, `F`, 于是`node new`选择和`node B`进行`peer`连接，开始下载，它一旦开始下载，自己本地也有`文件1`， 于是`node new`告诉`node C`以及和`node C`的`ID`很像的那些节点，我也有`文件1`了，可以加入那些文件拥有者列表了。但你会发现`node new`上没有文件索引，但根据哈希算法，一定会有某些文件的哈希值是和`node new`的`ID`匹配上的，在`DHT`网络中，会有节点告诉它，你既然加入这个网络，你也有责任知道某些文件的下载地址。一切都是分布式的。

里面有几个细节问题:
* **DHT node ID以及文件哈希值是什么东西？**：`节点ID`是一个随机选择的`160bits(20字节)`空间，文件的哈希也使用这样的`160bits`空间。
* **所谓ID相似，具体到什么程度算相似？**： 在`Kademlia`网络中，距离是通过异或(`XOR`)计算的
> `01010`与`01000`的距离就是两个`ID`之间的异或值，为`00010 = 2`, 高位不同表示距离更远，低位不同表示距离更近，总的距离为最后计算后的值。位置近不算近，只有`ID`近才算近。

`DHT`网络是按照**距离分层**的，`Kademlia`算法中，每个节点只有`4`个指令:
* **PING**: 测试一个节点是否在线
* **STORE**: 要求一个节点存储一份数据
* **FIND_NODE**: 根据`节点ID`查找一个节点
* **FIND_VALUE**: 根据`KEY`查找数据

`DHT`网络中，怎么更新节点数据？
* 每个`bucket`里的节点，都按最后一次接触时间倒序排列，相当于朋友圈最近联系的人是关系最好的
* 每次执行四个指令中的任一个都会触发更新
* 当一个节点与自己接触时，检查它是否已经在`k-bucket`中；如果在，将他移到`k-bucket`列表的最底(最新的位置)； 如果不在，新的联系人要不要加到通讯录里？如果通讯录满了，`PING`一下最旧的节点。如果`PING`通了，将旧节点移到列表最底，并丢弃新节点，如果`PING`不通，删除旧节点，并将新节点加入列表。

> 这个机制保证任意节点介入和离开都不影响整体网络。

## DNS 协议
`DNS`服务器一定要设置成高可用，高并发和分布式的。树状层级结构：
* 根`DNS`服务器: 返回顶级域 `DNS` 服务器的 `IP` 地址
* 顶级域`DNS`服务器: 返回权威`DNS` 服务器的 `IP` 地址
* 权威`DNS`服务器: 返回相应主机的`IP`地址

为了提高`DNS`解析性能， 很多网络多会就近部署`DNS`缓存服务器。`DNS`的解析流程如下：
* 电脑客户端发送一个`DNS`请求，问 [http://www.163.com](http://www.163.com) 的 `IP` 是啥？并发给本地域名服务器(本地`DNS`: 如果通过`DHCP`配置，本地`DNS`由网络服务商`ISP`,如电信，移动等自动分配，通常在网络服务商的某个机房)
* 本地`DNS`收到来自客户端的请求。这台服务器上缓存一张域名与与之对应`IP地址`的大表格, 如果能找到，直接返回`IP地址`. 如果没有，本地`DNS`会去根域名服务器找， 根域名服务器是最高层次， 全球共有`13`套。它不直接用于域名解析，但能知名一条道路。
* 根`DNS`收到来自本地`DNS`的请求，发现后缀是`.com`, 发现是由`.com`区域管理， 获得顶级域名服务器地址
* 本地`DNS`转向问顶级域名服务器，顶级域名服务器就是比如`.com`, `.net`, `.org` 这些一级域名，它负责管理二级域名， 比如`163.com`。
* 从顶级域名服务器获取[http://www.163.com](http://www.163.com)区域的权威`DNS`服务器地址
* 本地`DNS`转向权威`DNS`服务器，`163.com` 的权威`DNS`服务器是域名解析结果的源出处
* 权威`DNS`服务器查询后将相应的`IP地址` `X.X.X.X`告诉本地`DNS`.
* 本地`DNS`再将`IP地址`返回客户端，客户端和目标建立连接.

### 负载均衡
站在客户端的角度， 上面的过程是一次`DNS递归查询过程`。这个过程中，`DNS` 除了可以通过名称映射为`IP地址`，还可以实现**负载均衡**.

`DNS`首先可以做**内部负载均衡** , 还可以实现**全局负载均衡**。

> **DNS访问数据中心中对象存储上的静态资源**：假设全国有多个数据中心，托管在多个运营商， 每个数据中心有三个可用区(`Available Zone`)。对象存储通过跨可用区部署，实现高可用性。 在每个数据中心中， 都至少部署两个内部负载均衡器，内部负载均衡器后面对接多个对象存储的前置服务器。
* 当一个客户端要访问`object.yourcompany.com`时，需要将域名转换为`IP地址`进行访问，所以要请求`本地DNS解析器`
* `本地DNS解析器`先查看本地的缓存是否有这个记录。如果有直接使用
* 如果本地无缓存，则需要请求`本地DNS服务器`
* `本地DNS服务器`一般部署在你的数据中心或者你所在运营商的网络中，`本地DNS服务器`也需要看本地是否有缓存， 如果有则返回 
* 如果本地没有，`本地DNS` 才需要递归从`根DNS服务器`查到`.com`的顶级域名服务器，最终查到`yourcompany.com`的`权威DNS服务器`给`本地DNS服务器`，`权威DNS服务器`会返回真实要访问的`IP地址`

## HTTPDNS
`DNS`有两项功能：
* 根据名称查看具体地址
* 可以针对多个地址做负载均衡，而且可以选择一个离你最近的地址访问

有时`DNS`也会指错路：当我们发出请求解析`DNS`时，首先会先连接到运营商本地的`DNS`服务器，有这个服务器帮我们去整颗`DNS`树上进行解析，然后将解析结果返回给客户端。但是本地导游有自己的"小心思":
* **域名缓存问题**: 它可以本地做一个缓存，不是每个请求都会去访问权威`DNS`服务器，而是访问过一次就把结果缓存到自己本地，当其他人来时，直接返回缓存数据。
* **域名转发问题**: 缓存问题还本地域名解析服务，还是会去权威`DNS`服务器中查找，只不过不是每次都要查找。可以说这还是大导游，大中介。还有一些小导游，小中介，有了请求之后，直接转发给其他运营商去做解析，自己只是外包出去。
> 这样的问题是，如果是`A运营商`的客户，访问自己运营商的`DNS服务器`，如果`A运营商`去权威`DNS`服务器查询的话，会返回一个部署在`A运营商`的网站地址，这样针对相同运营商的访问，速度就会快很多。

> 但是`A运营商`偷懒，将解析的请求转发给`B运营商`, `B运营商`去权威`DNS`服务器查询的话，会返回一个在`B运营商`的网站地址，客户每次都要跨运营商访问，速度会慢很多

* **出口NAT问题**: 我们知道，出口时，很多机房都会配置`NAT`(网络地址转换), 使得从这个网管出去的包，都换成新的`IP地址`， 当然请求返回时，在这个网关，再将`IP地址`转换回去，所以对于访问来说没有任何问题。但是一旦做了网络地址转换，权威`DNS`服务器，就没办法通过这个地址来判断客户到底是来自哪个运营商，而且极有可能因为转换后的地址误判运营商，导致跨运营商的访问。
* **域名更新问题**: 本地`DNS`服务器由不同地区，不同运营商独立部署的。对域名解析缓存的处理上，实现策略也有区别，有的会偷懒，忽略域名解析结果的`TTL`时间限制，在权威`DNS`服务器解析变更时，解析结果在全网生效的周期非常漫长。但有时，在`DNS`的切换中，场景多生效时间要求比较高。
> 例如双机房部署时，跨机房的负载均衡和容灾多使用`DNS`来做。当一个机房出问题之后，需要修改权威`DNS`, 将域名指向新的`IP`地址，如果更新太慢，很多用户会出现访问异常。
* **解析延迟问题**: `DNS`查询过程需要递归遍历多个`DNS`服务器，才能获得最终的解析结果，这会带来一定的时延，甚至会解析超时。

### HTTPDNS的工作模式
`HTTPDNS`其实就是不走传统的`DNS`解析，而是自己搭建基于`HTTP`协议的`DNS`服务器集群，分布在多个地点和多个运营商。当客户需要`DNS`解析时，直接通过`HTTP`协议进行请求这个服务器集群，得到就近的地址。

这就相当于每家基于`HTTP`协议，自己实现自己的域名解析，自己做一个自己的地址簿，而不使用统一的地址簿。但是默认的域名解析都是走`DNS`的，因而使用`HTTPDNS`需要绕过默认的`DNS`路径，就不能使用默认的客户端。使用`HTTPDNS`的，往往是手机应用，需要在手机端嵌入支持`HTTPDNS`的客户端`SDK`。

通过自己的`HTTPDNS`服务器和自己的`SDK`，实现了从依赖本地导游，到自己上网查询做旅游攻略，进行自由行。这样就能避免依赖导游，而导游又不专业的尴尬。

**HTTPDNS的工作模式**:
> 在客户端`SDK`里动态请求服务器，获取`HTTPDNS`服务器的`IP`列表，缓存到本地。随着不断解析域名，`SDK`也会在本地缓存`DNS`域名解析的结果。当手机应用要访问一个地址时，首先查看是否有本地缓存，如果有就直接返回。这个缓存和本地`DNS`的缓存不一样的是，这个是手机应用自己做的，而不是这个运营商统一做的。如何更新，何时更新，手机应用的客户端可以和服务器协调来做这件事情。

> 如果本地没有，就需要请求`HTTPDNS`的服务器，在本地`HTTPDNS`服务器的`IP`列表中，选择一个发出`HTTP`的请求，会返回一个要访问网站的`IP`列表。

```plain
curl http://106.2.xxx.xxx/d?dn=c.m.163.com

{
    "dns": [
    {
        "host":"c.m.163.com",
        "ips":["223.252.199.12"],
        "ttl":300,
        "http2":0
    }],
    "client": {
        "ip":"106.2.81.50",
        "line":269692944
    }
}
```

手机客户端自然知道手机在哪个运营商，哪个地址。由于是直接的`HTTP`通信，`HTTPDNS`服务器能够准确知道这些信息，因而可以做精准的全局负载均衡。
![HTTPDNS](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3834.JPG  "HTTPDNS")
当然，当所有这些都不工作时，可以切换到传统的`LocalDNS`来解析，那`HTTPDNS`是如何解决上面的问题的？

> 其实归结起来就是两大问题。1.解析速度和更新速度的平衡问题 2.智能调度问题，对应的解决方案是`HTTPDNS`的缓存设计和调度设计。

**HTTPDNS的缓存设计**: 解析`DNS`过程复杂，通信次数多，对解析速度造成很大影响。为了加快解析，因而有了缓存，但是这又回产生缓存更新速度不及时的问题。最要命的是，这两个方面都掌握在别人的手中，也即本地`DNS`服务器手中，它不会为你定制。而`HTTPDNS`就是将解析速度和更新速度全部掌握在自己手中。一方面，解析的过程，不需要本地`DNS`服务器递归调用一大圈，一个`HTTP`的请求直接搞定，要实时更新时，马上就能起作用；另一方面为了提高解析速度，本地也有缓存，缓存实在客户端`SDK`维护的，过期时间，更新时间都可以自己控制。

`HTTPDNS`的缓存设计策略也是咱们做应用架构中常用的缓存设计模式，也即分为客户端，缓存，数据源三层：
* 对于应用架构来讲，就是应用，缓存，数据库。常见的是`Tomcat`, `Redis`, `MySQL`
* 对于`HTTPDNS`来讲，就是手机客户端，`DNS`缓存，`HTTPDNS`服务器。只要是缓存模式，就存在缓存过期，更新，不一致的问题，解决思路也很像。
> 例如`DNS`缓存在内存中，也可以持久化到存储上，从而`APP`重启之后，能够尽快从存储中加载上次累计的经常访问的网站的解析结果，就不需要每次全部解析一遍，再变成缓存。这有点像`Redis`是基于内存的缓存，但同样提供持久化的能力，使得重启或主备切换时，数据不会完全丢失。

> `SDK`中缓存会严格按照缓存过期时间，如果缓存没有命中，或者已经过期，而且客户端不允许使用过期的记录，则会发起一次解析 ，保障记录是更新的。解析可以**同步进行**，也可以直接调用`HTTPDNS`接口，返回最新的记录，更新缓存；也可以**异步进行**,添加一个解析任务到后台，由后台任务调用`HTTPDNS`接口。

> **同步更新**的有点是实时性好，缺点是如果有多个请求都发现过期时，同时会请求`HTTPDNS`多次。同步更新的方式对应到应用架构中缓存的`Cache-Aside 机制`, 也即先读缓存，不命中读数据库，同时将结果写入缓存。

> **异步更新**的优点是，可以同时在多个请求都发现过期的情况下，合并为一个对于`HTTPDNS`的请求任务，只执行一次，减少`HTTPDNS`的压力。同时可以在即将过期时，就创建一个任务进行预加载，防止过期之后再刷新。它的**缺点**是当前请求拿到过期数据时，如果客户端允许使用过期数据，需要冒一次险。如果过期数据还能请求，就没问题；如果不能请求，则失败一次，等下次缓存更新后，再请求才能成功。

异步更新机制对应到应用框架中缓存的`Refresh-Ahead 机制`, 即业务仅访问缓存，当过期时定期刷新。在著名的应用缓存`Guava Cache`中，有个`RefreshAfterWrite`机制，对于并发情况下，多个缓存访问不命中从而引发并发回源的情况。可以采用只有一个请求回源的模式。在应用架构的缓存中，也常常用`数据预热`或`预加载`机制。

### HTTPDNS的调度设计
由于客户端嵌入`SDK`, 因而就不会因为本地`DNS`的各种缓存，转发，`NAT`,让权威`DNS`服务器误会客户端所在的位置和运营商，可以拿到第一手资料。在客户端，可以知道手机是哪个国家，哪个运营商，哪个省，甚至哪个市，`HTTPDNS`服务器可以根据这些信息，选择最佳的服务节点返回。

如果有多个节点，还会考虑错误率，请求时间，服务器压力，网络状况等，进行综合选择，而非仅仅考虑地理位置。当有一个节点宕机或性能下降时，可以尽快进行切换。要做到这一点，需要客户端使用`HTTPDNS`返回的`IP`访问业务应用。客户端`SDK`回收集网络请求数据，如错误率，请求时间等网络请求质量数据，并发送到统计后台，进行分析，聚合，以此查看不同`IP`的服务质量。

![HTTPDNS](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3835.JPG "HTTPDNS")

在**服务端**, 应用可以通过调用`HTTPDNS`的管理接口，配置不同服务质量的优先级，权重。`HTTPDNS`会根据这些策略综合地理位置和线路状况算出一个排序，优先访问当前那先优质的，时延低的`IP`地址。`HTTPDNS`通过智能调度之后返回的结果，也会缓存在客户端。为了不让缓存使得调度失真，客户端可以根据不同的移动网络运营商`WIFI`的`SSID`来分维度缓存。不同运营商或`WIFI`解析出来的结果会不同。

## CDN
当一个用户想访问一个网站时，指定这个网站的域名，`DNS`就会将这个域名解析为地址，然后用户请求这个地址，返回一个网页。**那这里面还有没有可以优化的地方？**

> 例如去电商网站下单买个东西，这个东西一定要从电商总部的中心仓库送过来？原来基本是这样的，每一单都是单独配送，所以可能要很久才能收到东西。但是后来电商网站的物流在全国各地建立了很多仓库，而不是只有总部的中心仓库才可以发货。 

**我们先说，我们的网站访问可以借鉴"就近配送"这个思路**: 全球有这么多的数据中心，无论在哪里上网，临近不远的地方基本都有数据中心。是不是可以在这些数据中心里部署几台机器，形成一个缓存的集群来缓存部分数据，那么用户访问数据时，就可以就近访问?

当然是可以的，这些分布在各个地方的各个数据中心的节点，就称为**边缘节点**。由于边缘节点数目比较多，但是每个集群规模比较小，不可能缓存下来所有东西，因而可能无法命中，这样就会在边缘节点之上，有区域节点，规模就要更大，缓存的数据会更多，命中的概率也就更大。在区域节点之上是中心节点，规模更大，缓存数据更多。如果还不命中，就只好回源网站访问了。

![CDN](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3836.JPG "CDN")

这就是**CDN的分发系统架构**。`CDN`系统的缓存，也是一层层的，能不访问后端真正的源，就不打扰他。这也是电商网站物流系统的思路。有了这个分发系统之后，接下来就是，**客户端如果找到相应的边缘节点进行访问？**

还记得我们讲过的基于`DNS`的全局负载均衡吗？这个负载均衡主要用来选择一个就近的同样运营商的服务器进行访问。你会发现，`CDN`分发网络也是一个分布在多个区域，多个运营商的分布式系统，也可以用相同的思路选择最合适的边缘节点。

![CDN](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3837.JPG "CDN")

**在没有CDN的情况下**, 用户向浏览器输入`www.web.com`这个域名，客户端访问本地`DNS`服务器时，如果本地`DNS`服务器有缓存，则返回网站地址；如果没有，递归查询到网站的权威`DNS`服务器，这个权威`DNS`服务器时负责`web.com`的，它回返回网站的`IP`地址。本地`DNS`服务器缓存下`IP`地址，将`IP`地址返回，然后客户端直接访问这个`IP`地址，就访问到了这个网站。

然而**有了CDN之后，情况发生了变化。**在`web.com`这个权威`DNS`服务器上，会设置一个`CNAME`别名，指向另外一个域名`www.web.cdn.com`，返回给本地`DNS`服务器。当本地`DNS`服务器拿到这个新的域名时，需要继续解析这个新的域名。这时，再访问的就不是`web.com`的权威`DNS`服务器了，而是`web.cdn.com`的权威`DNS`服务器，这是`CDN`自己的权威`DNS`服务器，在这个服务器上，还是会设置一个`CNAME`, 指向另外一个域名，也即`CDN`网络的全局负载均衡器。

接下来，本地`DNS`服务器去请求`CDN`的全局负载均衡器解析域名，全局负载均衡器会为用户选择一台合适的缓存服务器提供服务，选择的依据包括:
* 根据用户`IP`地址，判断哪一台服务器距用户最近
* 用户所处的运营商
* 根据用户所请求的`URL`中携带的内容名称，判断哪一台服务器上有用户所需的内容
* 查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力

基于以上这些条件，进行综合分析之后，全局负载均衡器会返回一台缓存服务器的`IP`地址。本地`DNS`服务器缓存这个`IP`地址，然后将`IP`返回给客户端，客户端去访问这些边缘节点，下载资源。缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器上并没有用户想要的内容，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地。

**CDN可以进行缓存的内容有很多种**：保质期长的日用品比较容易缓存，因为不容易过期，对应到就像电商仓库系统里，就是静态页面，图片等，因为这些东西也不怎么变，所以适合缓存。

![CDN](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3838.JPG "CDN")

还记得这个**接入层缓存的架构吗？** 在进入数据中心时，我们希望通过最外层接入层的缓存，将大部分静态资源的访问拦在边缘。而`CDN`则是更进一步，将这些静态资源缓存到离用户更近的数据中心外。越接近客户，访问性能越好，时延越低。

但是静态内容中，有一种特殊的内容，也大量使用`CDN`, 这就是前面讲过的`流媒体`。`CDN`支持**流媒体协议**, 例如前面讲过的`RTMP`协议。在很多情况下，这相当于一个代理，从上一级缓存读取内容，转发给用户。由于流媒体往往是连续的，因而可以进行预先缓存的策略，也可以预先推送到用户的客户端。

对于静态页面来说，内容的分发往往采取**拉取**的方式，也即当发现微命中时，再去上一级进行拉取。但是，流媒体数据量大，如果出现**回源**, 压力回比较大，所以往往采取主动**推送**的模式，将热点数据主动推送到边缘节点。

对于流媒体来说，很多`CDN`还提供**预处理服务**，也即文件在分发之前，经过一定的处理。例如将视频转换为不同的码流，以适应不同网络带宽的用户需求；再比如对视频进行分片，降低存储压力，也使得客户端可以选择使用不同的码率加载不同的分片。

对于流媒体`CDN`来说，有个关键的问题是**防盗链**问题。因为视频是要花大价钱买版权的，为了挣点钱， 收点广告费，如果流媒体被其他的网站盗走，在人家的网站播放，那就损失惨重了；最常用也是最简单的方式就是**HTTP头的refer字段**，当浏览器发送请求时，一般会带上`refer`, 告诉服务器是从哪个页面链接过来的，服务器基于此可以获得一些信息用于处理。如果`refer`信息不是来自本站，就阻止访问或跳到其他链接。

> **refer的机制相对比较容易破解，所以还需要配合其他的机制**

一种常用的机制是**时间戳防盗链**。使用`CDN`的管理员可以在配置界面上和`CDN`厂商约定一个加密字符串。客户端取出当前的时间戳，要访问的资源及其路径，连同加密字符串进行签名算法得到一个字符串，然后生成一个下载链接，带上这个签名字符串和截止时间戳去访问`CDN`。在`CDN`服务器，根据取出过期时间和当前`CDN`节点时间进行比较，确认请求是否过期。然后`CDN`服务端有了资源及路径，时间戳，以及约定的加密字符串，根据相同的签名算法计算签名，如果匹配则一致，访问合法，才会将资源返回给客户。

然而比如在电商仓库中，我在前面提过，有关生鲜的缓存就是非常麻烦的，这对应的是动态的数据。比较难以缓存。现在也有**动态CDN, 主要有两种模式**:
* **生鲜超时模式**， 即**边缘计算模式**。既然数据是动态生成的，所以数据的逻辑计算和存储，也相应的放在边缘节点。其中定时从源数据那里同步存储的数据，然后在边缘进行计算得到结果。就像对生鲜的烹饪是动态的，也没办法事先做好缓存，因而将生鲜超市放在你家旁边，既能送货上门，也能现场烹饪，也是边缘计算的一种体现。
* **冷链运输模式**, 即**路径优化模式**。数据不是在边缘计算生成的，而是在源站生成的，但是数据的下发则可以通过`CDN`的网络，对路径进行优化。因为`CDN`节点较多，能够找到离用户很近的边缘节点。中间的链路完全由`CDN`来规划，选择一个更加可靠的路径，使用类似专线的方式进行访问。对于常用的`TCP`连接，在公网上传输时经常会丢数据，导致`TCP`的窗口始终很小，发送速度上不去。根据前面的`TCP`流量控制和拥塞控制的原理，在`CDN`加速网络中可以调整`TCP`的参数，使得`TCP`可以更加激进地传输数据。可以通过多个请求复用一个连接，保证每次动态请求到达时。连接都已经建立了，不必临时三次握手或建立过多的连接，增加服务器的压力。另外，可以通过对传输数据进行压缩，增加传输效率。所有这些手段就像冷链运输，整个物流优化了，全程冷冻高速运输。不管生鲜是从你旁边的超市送到你家的，还是从产地送的，保证到你家是新鲜的。

## 数据中心
无论是看新闻，下订单，看视频，下载文件，最终访问的目的地都在数据中心里。**数据中心**是一个大杂烩，几乎用到前面所学的所有知识。讲办公室网络时，我们知道办公室里有很多电脑，如果要访问外网，需要经过一个叫**网关**的东西，而网关往往是一个路由器。

**数据中心**里也有一大堆电脑，但它和我们办公室里的笔记本或台式机不一样。数据中心里是服务器。服务器被放在一个个叫做**机架(Rack)**的架子上。数据中心的入口和出口也是路由器，由于在数据中心的边界，就像在一个国家的边境，称为*边界路由器(Border Router)***。为了高可用，边界路由器会有多个。

一般家里只会连接一个运营商的网络，而为了高可用，为了一个运营商出问题时，还可以通过另外一个运营商提供服务，所以数据中心的边界路由器会连接多个运营商网络。既然是路由器，就需要跑路由协议，数据中心往往就是路由协议中的自治区域(`As`)。数据中心里的机器要想访问外面的网站，数据中心里也是有对外提供服务的机器，都可以通过`BGP`协议，获取内外互通的路由信息。这就是我们常听到的**多线BGP**的概念。

如果数据中心非常简单，没几台机器，那就像家里或宿舍一样，所有的服务器都直接连到路由器上就可以了。但是数据中心里往往有非常多的机器。当塞满一机架时，需要有交换机将这些服务器连接起来，可以互相通信。

这些交换机往往是放在机架顶端的，所以经常称为**TOR(Top Of Rack)交换机**。这一层的交换机常常称为**接入层(Access Layer)**。注意这个接入层和原来讲过的应用的接入层不是一个概念。

![数据中心](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3839.JPG "数据中心")

当一个机架放不下时，就需要多个机架，还需要有交换机将多个机架连接在一起。这些交换机对性能的要求更高，带宽也更大。这些交换机称为**汇聚层交换机(Aggregation Layer)**。数据中心里每一个连接都是需要考虑高可用的。这里首先要考虑的是，如果一台机器只有一个网卡，上面连着一条网线，接入到`TOR`交换机上。如果网卡坏了，或者不小心网线掉了，机器就上不去了。所以，需要至少两个网卡，两个网线插到`TOR`交换机上，但是两个网卡要工作的像一张网卡一样，这就是常说的**网卡绑定(bond)**。

这就需要服务器和交换机都支持一种协议**LACP(Link Aggregation Control Protocol)**。他们互相通信，将多个网卡聚合称为一个网卡，多个网线聚合成一个网线，在网线之间可以进行负载均衡，也可以为了高可用做准备。

![LACP](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3840.JPG "LACP")

网卡有了高可用保证，但交换机还有问题。如果一个机架只有一个交换机，它挂了，那整个机架都不能上网了。因而`TOR`交换机也需要高可用，同理接入层和汇聚层的连接也需要高可用性，也不能单线连着。最传统的方法是，部署两个接入交换机，两个汇聚交换机。服务器和两个交换机都连接，接入交换机和两个汇聚交换机都连接，当然这样会形成环，所以需要启用`STP`协议，去除环，但是这样两个汇聚就只能一主一备。`STP`协议里只有一条路会其作用。

![STP](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3841.JPG "STP")

交换机有一种技术叫做**堆叠**，所以另一种方法是，将多个交换机形成一个逻辑的交换机，服务器通过多根线分配连到多个接入层交换机上，而接入层交换机多根线分别连接到多个交换机上，并且通过堆叠的私有协议，形成**双活**的连接方式。

![堆叠](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3842.JPG "堆叠")

由于对带宽要求更大，而且挂了影响也更大，所以两个堆叠可能就不够了，可以就会有更多的，比如四个堆叠为一个逻辑的交换机。汇聚层将大量的计算节点相互连接在一起，形成一个集群。在这个集群里，服务器之间通过二层互通，这个区域常称为一个`POD(Point Of Delivery)`，有时候也称为一个**可用区(Available Zone)**。

当节点数目再多时，一个可用区放不下，需要将多个可用区连在一起，连接多个可用区的交换机称为**核心交换机**。

![核心交换机](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3843.JPG "核心交换机")

核心交换机吞吐量更大，高可用要求更高，肯定需要堆叠，但往往仅仅堆叠，不足以满足吞吐量，因而还是需要部署多组核心交换机。核心和汇聚交换机之间为了高可用，也是全互联模式的。

**这时出现环路怎么办？**：一种方式是，不同的可用区在不同的二层网络，需要分配不同的网段。汇聚和核心之间通过三层网络互通，二层都不在一个广播域里，不会存在二层环路的问题。三层有环是没有问题的，只要通过路由协议选择最佳的路径就可以了。**那为啥二层不能有环路，三层可以呢？**

![OSPF](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3844.JPG "OSPF")

如图，核心层和汇聚层之间通过内部的路由协议`OSPF`, 找到最佳的路径进行访问，而且还可以通过`ECMP`等价路由，在多个路由之间进行负载均衡和高可用。但是随着数据中心里的机器越来越多，尤其是有了云计算，大数据，集群规模非常大，而且都要求在一个二层网络里。这就需要二层互连从**汇聚层**上升为**核心层**, 也即在核心以下，全部是二层互连，全部在一个广播域里，这就是常说的**大二层**。

![大二层](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3845.JPG "大二层")

如果大二层横向流量不大，核心交换机数目不多，可以做堆叠，但如果横向流量很大，仅仅堆叠满足不了，就需要部署多组核心交换机，而且要和汇聚层进行全互连。由于堆叠只解决一个核心交换机组内的无环问题，而组之间全互连，还需要其他机制进行解决。如果是`STP`，那部署多组核心无法扩大横向流量的能力，因为还是只有一组起作用。

于是大二层就引入了**TRILL(Transparent Interconnection of Lots of Link)**，即**多链路透明互联协议**。它的基本思路是二层环有问题，三层环没有问题，就把三层的路由能力模拟在二层实现。运行`TRILL`协议的交换机称为`RBridge`, 是**具有路由转发特性的网桥设备**，只不过这个路由是根据`MAC`地址来的，不是根据`IP`来的。

`RBridage`之间通过**链路状态协议**运作。通过它可以学习整个大二层的拓扑，知道访问哪个`MAC`应该从哪个网桥走；还可以计算最短路径，也可以通过等价的路由进行负载均衡和高可用性。

![TRILL](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3846.JPG "TRILL")

`TRILL`协议在原来的`MAC`头外加上自己的头，以及外层的`MAC`头。`TRILL`头里的`Ingress RBridge`，有点像`IP`头里的源`IP`地址，`Egress RBridge`是目标`IP`地址，这两个地址是端到端的，在中间路由时，不会发生改变。而外层`MAC`, 可以有吓一跳的`Bridge`, 就像路由的下一跳，也是通过`MAC`地址来呈现的。

上图所示的过程，有一个包要从主机`A`发送到主机`B`, 中间经过`RBridge1`, `RBridge2`, `RBridge X`等，直到`RBridge 3`。在`RBridge 2`收到的包里，分内外两层，内层就是传统的主机`A`和主机`B`的`MAC`地址以及内层的`VLAN`。

在外层首先加上一个`TRILL`头，里面描述这个包从`RBridge 1`进来的，要从`RBridge 3`出去，并且像三层的`IP`地址一样有跳数。然后再外面，目的`MAC`是`RBridge 2`, 源`MAc`是`RBridge 1`, 以及外层的`VLAN`。

当`RBridge 2`收到这个包之后，首先看`MAC`是否是自己的`MAC`, 如果是，要看自己是不是`Egress RBridge`,也即是不是最后一跳；如果不是，查看跳数是不是大于`0`, 然后通过类似路由查找的方式找到下一跳`RBridge X`, 然后将包发出去。

`RBridge 2`发出去的包，内层的信息是不变的，外层的`TRILL`头里。同样，描述这个包从`RBridge 1`进来，要从`RBridge 3`出去，但是跳数要`-1`。外层的目标`MAC`变成`RBridge X`, 源`MAC`变成`RBridge 2`。

如此一直转发，直到`RBridge 3`, 将外层解出来，发送内层的包给主机`B`。

对于大二层的广播包，也需要通过分发树的技术来实现。我们知道`STP`是将一个有环图，通过去掉边形成一棵树，而分发树是一个有环的图形成多棵树，通过的树有不同的`VLAN`,有的广播包从`VLAN A`广播，有的从`VLAN B`广播，实现负载均衡和高可用。

![分发树](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3847.JPG "分发树")

核型交换机之外，就是边界路由器了。至此从服务器到数据中心边界的层次情况已经清楚了。在核心交换机上，往往会挂一些安全设备，例如入侵检测，`DDoS`防护等。这是整个数据中心的屏障，防止来自外来的攻击。核心交换机上往往还有负载均衡器。

在有的数据中心里，对于存储设备，还会有一个存储网络，用来连接`SAN`和`NAS`。但对于新的云计算来说，往往不使用传统的`SAN`和`NAS`，而使用部署在`x86`机器上的软件定义的存储，这样存储也是服务器了，而且可以和计算节点融合在一个机架上，从而更加有效率，也就没有了单独的存储网络了。

![数据中心](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3848.JPG "数据中心")

这是一个典型的三层网络结构。这里的三层不是指`IP`层，而是指接入层，汇聚层，核心层三层。这种模式非常有利于外部流量请求到内部应用。这个类型的流量，是从外到内或从内到外，对应到上图，就是从上到下，从下到上，上北下南，所以称为**南北流量**。

但是随着云计算和大数据的发展，节点之间的交互越来越多，例如大数据计算经常在不同节点将数据拷来拷去，这样需要经过交换机，使得数据从左到右，从右到左，左西右东，所以称为**东西流量**。为了解决东西流量问题，演进出了**叶脊网络(Spine/Leaf)**:
* **叶子交换机(Leaf)**, 直接连接物理服务器。`L2/L3`网络的分界点在叶子交换机上，叶子交换机之上是三层网络。
* **脊交换机(spine switch)**, 相当于核心交换机。叶脊之间通过`ECMP`动态选择多条路径。脊交换机现在只是为叶子交换机提供一个弹性的`L3`路由网络。南北流量可以不用直接从脊交换机发出，而是通过与`Leaf`交换机并行的交换机，再接到边界路由器出去。

![叶脊网络](http://7xpgi9.com1.z0.glb.clouddn.com/IMG_3849.JPG "叶脊网络")

传统的三层网络架构是垂直结构，而叶脊网络架构是扁平的结构，更易于水平扩展。
